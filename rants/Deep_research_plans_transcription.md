# File Metadata
- **Filename:** Deep research plans.m4a
- **File Created:** 2025-05-29 22:40:36
- **File Modified:** 2025-05-29 22:40:36
- **Processing Date:** 2025-05-29 23:04:05
- **File Size:** 15.31 MB
- **File Path:** Deep research plans.m4a

*Note: This metadata is generated from the file properties and may not reflect the actual date/time when the content was recorded.*


# Meeting Description




# Audio Analysis




# Visual Analysis




# Full Transcription



~[Speaker 1]~: Okay. So, let's talk first about the Let's talk about the first big one, namely kind of doing some reinforcement learning. Um so, in order to do this, I am going to have to first start off by firing a couple of deep research questions off.

Okay. The first deep research question is going to be around what is reinforcement learning when it comes to LLM models? How How does the math work? My my understanding is that reinforcement learning is basically takes the LLM, gives it a bunch of questions and answers where there is a lot of thinking or reasoning happening before the answer being given directly. And so you take it, you're reinforcement learning because you are taking try taking the end result at the end of all the thinking. And depending on how good that is, going back and giving smaller like rewards to all of the intermediate steps in between. Um first thinking, in between kind of the first token to the last token. This is very useful because you're kind of going, okay, so we know that putting more thought into the answer gives us better results. This is something that we've kind of tested and proven in both models and humans that just kind of when people think through something, they generally think about it better than the immediate reaction. Um This is kind of the system one, system two thinking that is popularized in the book Thinking Fast and Slow. And it like makes sense when you're talking about humans, but it's kind of interesting to see that given more time to reason it out, like models will too. But what coming off of that, we have seen and learned that there is different ways to think about something. Just thinking about something and then kind of giving the answer is all well and good. However, there are different mechanisms of thinking that can produce better or worse answers. Um And you can reason about this just using human thought processes. If you're trying to do something creative as a human, it often helps to actually think through multiple different options and then narrow it down to brainstorm something. Um whereas an alternative is to just kind of get started with one thing and then continually make small iterations and refinements on it. Um if you're trying to reason out how to build some how to architect some code, it's reasonably similar in that brainstorming a couple of different iterations is helpful. Um making kind of thoughtful predictions on expectations is also something that humans do. Like there's a lot of different ways to think. Um And so within that, like with models, there's also many different ways to think. Um And reinforcement learning is basically going, we want a mechanism that will teach the models like what good ways to think are. And the way we're going to do that is by taking some question and answer kind of complicated pairs, then we will get ask the model the question. Um it will have a bunch of thinking time to try out different um thought processes and then it will give the answer. We will we know the value function, the value function sorry, we know the reward um Fuck, I need to remember which one is which. I think it's that we know the reward for different types of um answers that some answers are good and some answers are bad. But then we need a value function to kind of work out what we're going to assign to each move um we made in getting to that answer to kind of like and this is going to make it so that instead of just telling the model and and that the answer itself was good or bad, we're actually spreading that goodness or badness out over the intermediate steps, the thinking process. Um And we're using this as basically a way to distill um out the thinking from the model. So we can give do training where we kind of give the models a bunch of different situations, a bunch of different question answer pairs and let it kind of And let it go where we just kind of go, okay, um train yourself such that you produce the good answer um for the question. Um except that in doing this training, the like goal of it and the mechanism of it is not so much to train the model to answer those questions or questions like it well. Instead, what we're training is that it we are trying to train the model to use the type of thinking that answers those questions well. Um to learn the policies that result in good answers. Um And this is kind of where the reinforcement learning part comes in that reinforcement learning is generally kind of going, okay, we're going to make an action, observed the situation, um and then make another action. So like part of my confusion around reinforcement learning um is that

Okay, so one of my confusions about reinforcement learning still is that I have been reading the reinforcement learning book um by I forget him. And that makes a very clear point in the introduction that reinforcement learning is not supervised learning and it's also not unsupervised learning. Basically, it was going that if you train the model and then use it, it's probably not that much reinforcement learning. At least that was what I got out of that book. Um reinforcement learning needs to have kind of the clear parts about reading the situation um about observing, sensing the situation, using that as input into the decision and policy to choose the next action. And then it kind of continuously updates the policy as it moves. And my impression of it um was very much that reinforcement learning was never finished training. But I don't know if I'm incorrect there or if I'm incorrect kind of with my understanding of RL with LLMs. Um Or I'm confused with my understanding of reinforcement learning when it comes to LLMs.



~[Speaker 1]~: Because yeah, I can I kind of got the argument for that the reinforcement LLM learning that I described earlier is reinforcement learning because it is spreading out the reward over all the steps in the value function. But on second thought, isn't that just kind of what like any neural network um gradient descent algorithm does? It like pushes the reward back into the other parts. So, okay, I guess my first question is very much what the fuck is reinforcement learning um for LLMs and what is reinforcement learning um more generally as laid out in and kind of clearly scoped in that one reinforcement learning book, which hopefully you know which one I'm talking about. I don't know if you do. Um But yeah, so that I think is a decent place to start. I hope that that kind of description makes it clear where my thinking is at, where my understanding is at, but also makes it I want to make sure it is incredibly, incredibly clear that I don't know what the fuck I'm talking about. Um and I am asking these questions in order and putting forward my understanding in order to refine and improve my understanding. So I am very much expecting to be corrected and challenged. Um but I need kind of you to know what you're talking about too. So please do some research. Make sure that like you're accurate. Okay. Um So I think that with this first section, that's a bit of a brain dump.


~[Speaker 1]~: Let's see if this can't be kind of tidied up into a my thoughts, my understanding, cover it and cover like my current understanding, break that down into all the different parts. Have a like this where I'm confused section and then also have a um questions section. And we will take that and give it to both deep research and normal Claude. Okay.


~[Speaker 1]~: The next item is going to be what Okay, new section, new section. This is the start of a new section. Okay, let's go. So the next question is going to be all around how do I do reinforcement learning for LLMs? What do I need? What are the packages? Um what are the stable packages? What are the popular packages? Um to do reinforcement learning in LLMs as of today. This will need a bunch of deep research going into it. Um I first kind of want a practitioner's guide, not an researcher's guide or an understanding guide, but like a I just fucking want to build it and I don't really care how things work. Okay, I do care how things work. Guide to what you need to do to get to reinforcement train an LLM. So again, we'll start this one off just by going to normal a normal model interaction and asking what's the general principle of things. But then we'll want to kind of have a deep research that expands out every element. So kind of what do you need um in order to reinforcement train an LLM. I'm guessing you need an open source base model, um a data set of some kind. And then also a framework or kind of library to do the maths for you. Um And so the first question is going to be kind of is that sufficient? What else do you need? Um and that question can just go to a normal LLM a normal chat. Um The next then after that, we'll have three at least three different deep researches based on the response from the last chat. Basically for everything that we need a deep research that goes into what's the most popular stable recommendations for this? What's the most a popular stable model to train? Um what are parameters around needing that you need to be aware of when selecting what type of model to train? What libraries and what data sources?